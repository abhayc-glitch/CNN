# The tanh function is considered more of an practice fucntion.
# YOu have to do the sigmoid to do this (KINDA DUMB)
# Sounds cool...

"""
It takes a real value as input and squashes it in the range(-1, 1).

In practice, the tanh activation is preferred over the sigmoid activation.
It is also common to use the tanh function in state to state transition models(recurrent neural networks).
The tanh function also suffers from the gradient saturation problem and kills gradients when saturated.
"""

# TRASHHHHHHH