What is a Neural Network?

Basically a neural network is a model that recongizes patterns in your data or data set.
It uses said patterns and makes a prediction.

The Network has many parts usch as layers, Activation fucntions, and neurons/nodes

"""
The ReLU is the most popular and commonly used activation function. It can be represented as –

f(x) = max(0, x)

It takes a real value as input. The output is x, when x > 0 and is 0, when x < 0. It is mostly preferred over sigmoid and tanh.

Advantages >>>>>>>>>>>>>>>>>>

Accelerates the convergence of SGD compared to sigmoid and tanh(around 6 times).
No expensive operations are required.

Disadvantages >>>>>>>>>>>>>>>

It is fragile and might die during training. If the learning rate is too high the weights may change to a value that causes the neuron to not get updated at any data point again.
"""
# Debating against the ReLU and the Leaky ReLU

# There are some interesting complications to the ReLU when it comes to optimizing weights and biases
# for ReLU...
# >>> IF the weigth and the bias are both = to 0 then the ReLU on a graph will be a straight line. The input can be 4 million but it doenst matter because the biases and the weights prevent it.
# >>>> If the weight is above 0 and the bias remains equal to 0, the steepness of the line on the graph will change. But the bend will still start at 0 because our bias is 0.
# >>>>> Now if the weight remains the same (1.00) but we change the bias to be 0.50 then activation of the function will happen sooner.
"""
 >>>>>> With a negative weight and this single neuron, the function has become a question of when this
neuron deactivates. Up to this point, you’ve seen how we can use the bias to offset the function
horizontally, and the weight to influence the slope of the activation. Moreover, we’re also able to
control whether the function is one for determining where the neuron activates or deactivates.

^^^^^ THE ABOVE ARE ALL WITH JUST ONE NEURON WE WILL EXPLORE MULTI-DIMENSIONAL ReLU ACTIVATIONS NEXT.
"""
# WITH MULTI-DIMENSIONAL neurons is were the magic happens.

"""
Now we see some fairly interesting behavior. The bias of the second neuron indeed shifted the
overall function, but, rather than shifting it horizontally, it shifted the function vertically. What
then might happen if we make that 2nd neuron’s weight -2 rather than 1?

What we have here is a neuron that has both an activation and a
deactivation point. When both neurons are activated, when their “area of effect” comes into play,
they produce values in the range of the granular, variable, and output. If any neuron in the pair is
inactive, the pair will produce non-variable output:

"""

# WILL USE A SNIPPET OF THIS CODE in the P.5

# WHen we import something as a name we are calling it sum
# Capital x is just a common practice dw


# The Softmax activation function.
# We will use this for the output layer.
# The main reason we need another function for the output layer is we need osmehting to create a probability to that the Network can make a prediction.
# We can make a prediction maybe correctly whenusing the ReLU and when we ahve all positive values but when we have a negative value, Relu clips negatives to zero.
# SO both neurons are negative the network outputs are both zero. SO it cant be make a prediction.
# Hence we need a function whos equation doesnt clip the output to a zero.
# YAYYYY softmax.
#  \sigma(\vec{z})_{i}=\frac{e^{z_{i}}}{\sum_{j=1}^{K} e^{z_{j}}}
# We also expionetiate the function so we can mkae them non negative.
# Then we get a very high number, so then we "normalize" them (divide them) so we get a vector of small values.
# THEN WE ADD IT UP and get hte final number f that neuron.

"""
With a randomly-initialized model, or even a model initialized with more sophisticated
approaches, our goal is to train, or teach, a model over time. To train a model, we tweak the
weights and biases to improve the model’s accuracy and confidence. To do this, we calculate how
much error the model has. The loss function, also referred to as the cost function, is the
algorithm that quantifies how wrong a model is. Loss is the measure of this metric. Since loss is
the model’s error, we ideally want it to be 0.
"""
# Basically it talks to the Netowrk and it tells it how bad it did. Like a test score.
# The lower the loss funciton outputs the better our network did.

# Preferably we could clssify a (True, False) and then tell the network how correct he is (50% cux only true or false).
# We cant do that becasue we want the network to be confident in tis answer.
# As you can probably guess there are diffrebt types of Loss functions
# The softmax function outputs a probability distro so we need to Calculate loss with something that only comapres prob distros
# CATAGORIAL CROSS ENTROPY >>>>
# Why Categorical Cross Entropy >>> when introducing back propogation it becomes easier to change the values.
# Categories are like groups on types of data.
# One-Hot encoding>>>>>>
# Each output class(OT layer neuron) doesnt retrun a lable saying whatver you wnat to output, yet it will output an integer.

# One Hot encdoing BASICALLY HELPS US SEE WHICH NEURON IS "HOT" or WHICH NEURON IS ACTIVATED.
# A vector is created for each category classes. All of the numbers are zeroes unless the coressponding class number's index in the vector is the number "1".
"""

softmax_output = [[0.7, 0.1, 0.2],
                  [0.1, 0.5, 0.4],
                  [0.02, 0.9, 0.08]]
"""
"""
The first value, 0, in class_targets means the first softmax output distribution’s intended
prediction was the one at the 0th index of [0.7, 0.1, 0.2]; the model has a 0.7 confidence
score that this observation is a dog. This continues throughout the batch, where the intended target
of the 2nd softmax distribution, [0.1, 0.5, 0.4], was at an index of 1; the model only has a
0.5 confidence score that this is a cat — the model is less certain about this observation. In the
last sample, it’s also the 2nd index from the softmax distribution, a value of 0.9 in this case — a
pretty high confidence
"""

# Introducing Optimization
# Optimization is a way to adjust the weights and biases to get a better result and to get higher accuracy abnd lower cost function value

"""
Randomly changing and searching for optimal weights and biases did not prove fruitful for one
main reason: the number of possible combinations of weights and biases is infinite, and we need
something smarter than pure luck to achieve any success. Each weight and bias may also have
different degrees of influence on the loss — this influence depends on the parameters themselves
as well as on the current sample, which is an input to the first layer. These input values are then
multiplied by the weights, so the input data affects the neuron’s output and affects the impact that
the weights make on the loss. The same principle applies to the biases and parameters in the next
layers, taking the previous layer’s outputs as inputs. This means that the impact on the output
values depends on the parameters as well as the samples — which is why we are calculating the
loss value per each sample separately. Finally, the function of how a weight or bias impacts the
overall loss is not necessarily linear. In order to know how to adjust weights and biases, we first
need to understand their impact on the loss.
"""
"""
The derivatives that we’ve solved so far have been
cases where there is only one independent variable in the function — that is, the result depended
solely on, in our case, x. However, our neural network consists, for example, of neurons, which
have multiple inputs. Each input gets multiplied by the corresponding weight (a function of 2
parameters), and they get summed with the bias (a function of as many parameters as there are
inputs, plus one for a bias). As we’ll explain soon in detail, to learn the impact of all of the inputs,
weights, and biases to the neuron output and at the end of the loss function, we need to calculate
the derivative of each operation performed during the forward pass in the neuron and the whole
model. To do that and get answers, we’ll need to use the chain rule.
"""
# The Partial derivative is a calculation of how much impact a neuron has on a function output.
# Basically like the rate of change.

# Gradients are just a 1d array / Vector of the partial derivatives of an input.

# Chain rule: The forward pass through our model is a chain of functions similar to these examples. We are passing in samples, the data flows through all of the layers, and activation functions to form a CHAIN.

# Chain Rule is kind of like the transitive property.
# Suppose we are a collecting the heights and the weights of someone.
# >> They would have a graph with a slope of 2/1 == 2. (For every 1 unit increased in weight, 2 units are increased into height.)
# SUppose we have another graph collecting the weights and shoe size.
# >> They would have another graph with a slope of (1/4). (For every 1 unit increased in height, 1/4 units are increased into shoe size.))
# So these two graphs are corresponding.
# Weight can predict height and height can predict shoe size.
# So we if we want to determine the exact changes with respect to changes in weight....
# WE can take the derivative of the equation if shoe size.....
#==================== (dshoesize/dweight) = (dheight/dweight) x (dshoesize/dheight)
# The relationship above is called the chain rule.

# Its used to help create a form of multiple function outputs and multiply them for the final outcome.

#VERY IMPORTANT-------------

"""
Derivatives of a function f(x):
    - how fast f changes around x (slope)
    - will f increase or decrease if we increse x
    - https://www.youtube.com/watch?v=3p09AyD-eWI


"""
"""
#=============BACKPROPAGATION(With Basic levels of Optimization)===============#
"""
# SUppose the network we are wokring it isnt trained (obviously).
# The network is supposes to recognize hand written images and We input a 2 but we get all sorts of wrong answers.
# We first find the loss of the network.
# Since we want the network to classify it as an image of a 2 but the output value of the "2" in the layer is low,....
# WE CAN NUDGE ALL THE weights AND BIASES UP FOR the "2" and nudge all the other neuron values down. And the the should be proportional to its size as well
#   (wrong answer: 6 ~ 0.9 >>> thsi should be nudged down more than the other ones because the network thinks this is the highest value.)

# WEIGHTS ONLY>>>>>
#   Once you go back you should retrace your steps to see which weights have the moist effect. And then you should change those.

# BIASES ONLY>>>>
#   Once you go back change the bias of the neuron in the prevoius layer that influences the Cost Function

# Basically you have a list of nudges to every neuron that you have to do.
# Then you recursivly apply the same properties to the first layer weights and biases.


# >>>>>>>>>>>>>> THE MAIN KEY IDEA IS THAT THIS WORKS FOR ONLY A 2 SO WE NEED TO DO THIS FOR ALL POSSIBLE OUTCOMES.


# Every SINGLE Training DATA needs to be back propagated.
"""
#=======================OPTIMIZERS====================#
"""
#Once we have calculated the gradient, we can use this information to adjust weights and biases to
#decrease the measure of loss. In a previous toy example, we showed how we could successfully
#decrease a neuron’s activation function’s (ReLU) output in this manner. Recall that we subtracted
#a fraction of the gradient for each weight and bias parameter. While very rudimentary, this is still
#a commonly used optimizer called Stochastic Gradient Descent (SGD). As you will soon
#discover, most optimizers are just variants of SGD.
#The first name, Stochastic Gradient Descent, historically refers to an optimizer that fits a single
#sample at a time. The second optimizer, Batch Gradient Descent, is an optimizer used to fit a
#whole dataset at once. The last optimizer, Mini-batch Gradient Descent, is used to fit slices of a
#dataset, which we’d call batches in our context. The naming convention can be confusing here for
#multiple reasons.


#When we iterativly print the epoch and its current iteration, it work but not that well. Te loss stays at 0.85 to 0.95
#OUt learning Rate is the cause of this.
#FOr the Stochastic Gradient Descent we need to use rabdomly selected mini batches.
"""
class SGD:
    # Make the optimizer
    # Intialize the learning rate which is 1.0 for now.
    def __init__(self, learning_rate=3.29):
        self.learning_rate = learning_rate
    # Update the Parameters of the network.

    def update(self, layer):
        layer.weights += -self.learning_rate * layer.dweights
        layer.biases += -self.learning_rate * layer.dbiases

# The learning rate is a Hyperparameter that shows how fast your netowrk can adapt to the the situation data.
# The lower it is the better. Its like a speed calculation.

In the code above the learning rate is 3.29 and it does well but not great.
since i cant try every value for the learning rate, i need to write a function to do that
Programmers are lazy 
"""
# When we write said program to make this we end up with Decay ----
# Decay is a way the learning rate can go down by itself
# Momentum is when we take the average if the last few steps taken and add that to the push the function outside of the local minuma.
"""
#============================ DataSets ============================#
"""
# Once we finish OPtmization and when we use data from the randomizer to train it. That is called TRAINING DATA
# But one we train it and the model is performing well...
# We need data that is not similar to the TRANING DATA and use that to validate the model to see how well it can perform with out of sample data. >>> THis is called Testing Data
# Its important to have 2 different sets for each piece.
# When we make outof smaple data we have to be adameant that no simiar data pictures or data crosses

#Overfitting happens when the Network becomes too good at predicting the Traning data, that it does bad when introduced to actual validation data.
# The basic reasoning here is that; the fewer neurons you have the theres a less chance that model is just memorizing .
# The Concept of genereralization is when the network can make an educated guess for every set of data to avoid overfitting

# IN the optimization we found the best parameters for the TRAINING DATA. THis is bad because again it causes overfitting.
# SO you need another data set called Validation data for tuning and changing the Hyperparameters.
# The test dataset needs to contain real out-of-sample data, but with a validation dataset, we have more
# freedom with choosing data. If we have a lot of training data and can afford to use some for
# validation purposes, we can take it as an out-of-sample dataset, similar to a test dataset. We can
# now search for parameters that work best using this new validation dataset and test our model at the
# end using the test dataset to see if we really tuned the model or just overfitted it to the validation data.

#There are situations when we’ll be short on data and cannot afford to create yet another dataset
#from the training data. In those situations, we have two options:
"""
The first is to temporarily split the training data into a smaller training dataset and validation
dataset for hyperparameter tuning. Afterward, with the final hyperparameter set, train the model
on all the training data. We allow ourselves to do that as we tune the model to the part of training
data that we put aside as validation data. Keep in mind that we still have a test dataset to check the
model’s performance after training.
"""

# The other technique is called CROSS VALIDATION.
# WHen we dont have enough resources.
# How it works is we split the training dataset into a
# given number of parts, let’s say 5. We now train the model on the first 4 chunks and validate it on
# the last. So far, this is similar to the case described previously — we are also only using the
# training dataset and can validate on data that was not used for training. What makes
# cross-validation different is that we then swap samples. For example, if we have 5 chunks, we can
# call them chunks A, B, C, D, and E. We may first train on A, B, C, and D, then validate on E.
# We’ll then train on A, B, C, E, and validate on D, doing this until we’ve validated on each of the
# 5 sample groups

#Training DataSet>>> basic stuff ya dig.
"""
In cases where we do not have many training samples, we could use data augmentation. One
easy way to understand augmentation is in the case of images. Let’s imagine that our model’s goal
is to detect rotten fruits — apples, for example. We will take a photo of an apple from different
angles and predict whether it’s rotten. We should get more pictures in this case, but let’s assume
that we cannot. What we could do is to take photos that we have, rotate, crop, and save those as
worthy data too. This way, we have added more samples to the dataset, which can help with model
generalization. In general, if we use augmentation, then it’s only useful if the augmentations that
we make are similar to variations that we could see in reality. For example, we may refrain from
using a rotation when creating a model to detect road signs as they are not being rotated in real-life
scenarios (in most cases, anyway).
"""

#======================L1 and L2 regularization=======================#
# Reguralization methods are those which reduce generalization.
# Normally when we have a generilization we have Biases that ARE HUGEE and to prevent that we can use...
# l1 and L2 regularization: which basically just fines the network and is added to the loss func(CCE)
# L1 is trash and it is liner since its the: sum of all the absolute values for the weights and biases
# L2 is the most commonly used and is non linear : e sum of the squared weights and biases
# This non-linear approach penalizes larger weights and biases more than smaller ones because of the square function used to calculate the result
# We use a value referred to as lambda in this equation — where a higher value means a more significant penalty.

# ====================== Droupouts======================================#
Another option for regularization is adding a droupout layer.
A dropout acts like a filer layer and lets some neurons pass and doesnt let other pass.
The Dropout function works by randomly disabling neurons at a given rate during every forward
pass, forcing the network to learn how to make accurate predictions with only a random part of
neurons remaining.
Dropout forces the model to use more neurons for the same purpose,
resulting in a higher chance of learning the underlying function that describes the data. For
example, if we disable one half of the neurons during the current step, and the other half during
the next step, we are forcing more neurons to learn the data, as only a part of them “sees” the data
and gets updates in a given pass.
We’ll use a hyperparameter to inform the dropout layer of the number of neurons to
disable randomly. It’s also worth mentioning that the dropout layer
does not truly disable neurons, but instead zeroes their outputs. In other words, dropout does not
decrease the number of neurons used, nor does it make the training process twice as fast when
half the neurons are disabled.
>>> Forward 
    With the code, we have one hyperparameter for a dropout layer. This is a value for the percentage
    of neurons to disable in that layer. For example, if you chose 0.10 for the dropout parameter, 10%
    of the neurons will be disabled at random during each forward pass. Before we use NumPy, we’ll
    demonstrate this with an example in pure Python. but the idea is to keep zeroing neuron outputs (setting them to
    0) randomly until we’ve disabled whatever target % of neurons we require.
        np.random.binomial(2, 0.5, size=10)
        This will produce an array that is of size 10, where each element will be the sum of 2 coin tosses,
        where the probability of 1 will be 0.5, or 50%. The resulting array:
        array([0, 0, 1, 2, 0, 2, 0, 1, 0, 2])


Until now, we’ve used an output layer that is a probability
distribution, where all of the values represent a confidence level of a given class being the correct
class, and where these confidences sum to 1. We’re now going to cover an alternate output layer
option, where each neuron separately represents two classes — 0 for one of the classes, and a 1
for the other. A model with this type of output layer is called binary logistic regression. This
single neuron could represent two classes like cat vs. dog, but it could also represent cat vs. not
cat or any combination of 2 classes, and you could have many of these. For example, a model
may have two binary output neurons. One of these neurons could be distinguishing between
person/not person, and the other neuron could be deciding between indoors/outdoors. Binary
logistic regression is a regressor type of algorithm, which will differ as we’ll use a sigmoid
activation function for the output layer rather than softmax, and binary cross-entropy rather than
categorical cross-entropy for calculating loss


Regression is a different approach to toher type of problems. 
FOr example if we have a network that is classying "WHAT THAT OBJECT IS " vs having a network deciding "WHAT THE TEMPRATURE."
The output has to be more granular.


Model Object organizing inclusdes making our 


